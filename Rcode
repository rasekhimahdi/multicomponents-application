###Application Section
######################
######################Goodness of fit test of data for GL distribution
GLOd<-function(x,alpha,lambda){
 G<-rep(0,0)
 for(i in 1:length(x)){
    G[i]<-alpha*lambda*exp(-lambda*x[i])*(1+exp(-lambda*x[i]))^(-alpha-1)
 }
return(G)
}
######Other model Comparison
library("AdequacyModel")
x<-c(1.901,2.132,2.203,2.228,2.257,2.350,2.361,2.396,
2.397,2.445,2.454,2.474,2.518,2.522,2.525,2.532,2.575,
2.614,2.616,2.618,2.624,2.659,2.675,2.738,2.740,2.856,
2.917,2.928,2.937,2.937,2.977,2.996,3.030,3.125,3.139,
3.145,3.220,3.223,3.235,3.243,3.264,3.272,3.294,3.332,
3.346,3.377,3.408,3.435,3.493,3.501,3.537,3.554,3.562,
3.628,3.852,3.871,3.886,3.971,4.024,4.027,4.225,4.395,5.020)
x<-(x-mean(x))/sd(x)
######
hist(x,prob=T,breaks=10,main="",ylim=c(0,1))
y<-seq(-5,5,.01)
#####OLLGG
cdf_GLO<-function(par,x){
alpha=par[1]
lambda=par[2]
  CDF=(1+exp(-lambda*x))^(-alpha)
  PDF=alpha*lambda*exp(-lambda*x)*(1+exp(-lambda*x))^(-alpha-1)
return(CDF)
  }
pdf_GLO<-function(par,x){
alpha=par[1]
lambda=par[2]
  CDF=(1+exp(-lambda*x))^(-alpha)
  PDF=alpha*lambda*exp(-lambda*x)*(1+exp(-lambda*x))^(-alpha-1)
return(PDF)
}
GLO<-goodness.fit(pdf=pdf_GLO, cdf=cdf_GLO, 
             starts = c(1,1), data = x,
             method="N", domain=c(0.1,Inf),mle=NULL)
lines(y,GLOd(y,GLO$mle[1],GLO$mle[2]),lwd=2,lty=1,col="black")
###########################
library("AdequacyModel")
x<-c(1.312,1.314,1.479,1.552,1.700,1.803,1.861,1.865,1.944,
1.958,1.966,1.997,2.006,2.021,2.027,2.055,2.063,2.098,2.140,
2.179,2.224,2.240,2.253,2.270,2.272,2.274,2.301,2.301,2.359,
2.382,2.382,2.426,2.434,2.435,2.478,2.490,2.511,2.514,2.535,
2.554,2.566,2.570,2.586,2.629,2.633,2.642,2.648,2.684,2.697,
2.726,2.770,2.773,2.800,2.809,2.818,2.821,2.848,2.880,2.954,
3.012,3.067,3.084,3.090,3.096,3.128,3.233,3.433,3.585,3.585)
x<-(x-mean(x))/sd(x)
######
hist(x,prob=T,breaks=10,main="",ylim=c(0,1))
y<-seq(-5,5,.01)
#####OLLGG
cdf_GLO<-function(par,x){
alpha=par[1]
lambda=par[2]
  CDF=(1+exp(-lambda*x))^(-alpha)
  PDF=alpha*lambda*exp(-lambda*x)*(1+exp(-lambda*x))^(-alpha-1)
return(CDF)
  }

pdf_GLO<-function(par,x){
alpha=par[1]
lambda=par[2]
  CDF=(1+exp(-lambda*x))^(-alpha)
  PDF=alpha*lambda*exp(-lambda*x)*(1+exp(-lambda*x))^(-alpha-1)
return(PDF)
}
GLO<-goodness.fit(pdf=pdf_GLO, cdf=cdf_GLO, 
             starts = c(1,1), data = x,
             method="N", domain=c(0.1,Inf),mle=NULL)
lines(y,GLOd(y,GLO$mle[1],GLO$mle[2]),lwd=2,lty=1,col="black")
########################################################################

##Generalized Logistic Multicomponents stress strengh

######################Classical method
MLERsk<-function(x,y){
G<-rep(0,0)
n<-length(x)
m<-length(y) 
likelihood<-function(par){
a1<-par[1]
a2<-par[2]
lambda<-par[3]
n<-length(x)
m<-length(y)    
    G<- -((n*log(a1))+(m*log(a2))+((n+m)*log(lambda))-(lambda*(sum(x)+sum(y)))
-(a1+1)*sum(log(1+exp(-lambda*x)))-(a2+1)*sum(log(1+exp(-lambda*y))))
return(G)
}
MLESTIM <- optim(c(alpha1,alpha2,lambdastar),likelihood,hessian=T,method="L-BFGS-B"
,lower = c(.1,.1,.1),upper =c(Inf,Inf,Inf))
a1<-MLESTIM$par[1]
a2<-MLESTIM$par[2]
lambda<-MLESTIM$par[3]
#####EStimate R12 and R23
G[1]<-(2*a1*a2)/(a2*(a2+(2*a1)))
G[2]<-(6*a1^2)/((a2+3*a1)*(a2+2*a1))
#####EStimate variance of R12 and R23
J<-matrix(c(0),nrow=3,ncol=3,byrow=T)
J[1,1]<-(n/(a1^2))
J[2,2]<-(m/(a2^2))
J[1,3]<--n*((psigamma(a1, deriv = 0)-psigamma(1, deriv = 0)-1)/(lambda*(a1+1)))
J[3,1]<-J[1,3]
J[2,3]<--m*((psigamma(a2, deriv = 0)-psigamma(1, deriv = 0)-1)/(lambda*(a2+1)))
J[3,2]<-J[2,3]
J[3,3]<-((n+m)/(lambda^2))+((n/((lambda^2)*(a1+2)))*((a1*(psigamma(a1, deriv = 1)
+(psigamma(a1, deriv = 0)^2)+(psigamma(1, deriv = 0)^2)+((pi^2)/6)))
+(2*(1-(a1*(psigamma(1, deriv = 0)+1)))*(psigamma(a1, deriv = 0)-1))))+
((m/((lambda^2)*(a2+2)))*((a2*(psigamma(a2, deriv = 1)+(psigamma(a2, deriv = 0)^2)+
(psigamma(1, deriv = 0)^2)+((pi^2)/6)))
+(2*(1-(a2*(psigamma(1, deriv = 0)+1)))*(psigamma(a2, deriv = 0)-1))))
u<-(J[1,1]*J[2,2]*J[3,3])-(J[1,1]*(J[2,3]^2))-(J[2,2]*(J[1,3]^2))
G[3]<-((4*a1^2)/(u*(a2+(2*a1))^4))*((((a2/a1)^2)*(J[2,2]*J[3,3]-(J[2,3])^2))-
((a2/a1)*J[1,3]*J[2,3])+(J[1,1]*J[3,3])-((J[1,3])^2))
G[4]<-((36*(a1^4)*((2*a2+5*a1)^2))/(u*((a2+3*a1)^4)*((a2+2*a1)^4)))*
((((a2/a1)^2)*(J[2,2]*J[3,3]-(J[2,3])^2))-((a2/a1)*J[1,3]*J[2,3])+(J[1,1]*J[3,3])
-((J[1,3])^2))
return(G)
}
###############################################
###importing data set
x<-c(1.901,2.132,2.203,2.228,2.257,2.350,2.361,2.396,2.397,2.445,2.454,2.474,2.518,
2.522,2.525,2.532,2.575,2.614,2.616,2.618,2.624,2.659,2.675,2.738,2.740,2.856,2.917,
2.928,2.937,2.937,2.977,2.996,3.030,3.125,3.139,3.145,3.220,3.223,3.235,3.243,3.264,
3.272,3.294,3.332,3.346,3.377,3.408,3.435,3.493,3.501,3.537,3.554,3.562,3.628,3.852,
3.871,3.886,3.971,4.024,4.027,4.225,4.395,5.020)
x<-(x-mean(x))/sd(x)

y<-c(1.312,1.314,1.479,1.552,1.700,1.803,1.861,1.865,1.944,1.958,1.966,1.997,2.006,
2.021,2.027,2.055,2.063,2.098,2.140,2.179,2.224,2.240,2.253,2.270,2.272,2.274,2.301,
2.301,2.359,2.382,2.382,2.426,2.434,2.435,2.478,2.490,2.511,2.514,2.535,2.554,2.566,
2.570,2.586,2.629,2.633,2.642,2.648,2.684,2.697,2.726,2.770,2.773,2.800,2.809,2.818,
2.821,2.848,2.880,2.954,3.012,3.067,3.084,3.090,3.096,3.128,3.233,3.433,3.585,3.585)
y<-(y-mean(y))/sd(y)
####initial value for optimization
alpha1<-2
alpha2<-2
lambdastar<-1.7
#####MLE and Variance of R12 and R23
MLERsk(x,y)
####################################
#####Final result of Classical Approach

#####MLE R12 and R23 with sd and confidence interval
MLERsk(x,y)[1]
sqrt(MLERsk(x,y))[3]
c((MLERsk(x,y)[1]-1.959964*sqrt(MLERsk(x,y)[3])) , MLERsk(x,y)[1]+1.959964*sqrt(MLERsk(x,y)[3]))

MLERsk(x,y)[2]
sqrt(MLERsk(x,y)[4])
c((MLERsk(x,y)[2]-1.959964*sqrt(MLERsk(x,y)[4])) , MLERsk(x,y)[2]+1.959964*sqrt(MLERsk(x,y)[4]))
###########################################################
#Bayesian method
rm(list=ls())

Inserting data
repeatn<-100
alfa10<-1.8 ; landa10<-2.5 ; alfa20<-2 ; landa20<-2.5
nsamplex<-5 ; nsampley<-5
a10<-3 ; a20<-5 ;a30<-1 ;a40<-2 ; b10<-3 ; b20<-3 ;b30<-6 ; b40<-1
sigma2<-.25
seed<-24000
nmh0<-1 ; ngs<-5000
alfa1s<-2.1 ; landa1s<-2.9 ; alfa2s<-1.15 ; landa2s<-2.9

Starting values
Programms
dlogis<-function(x,alfa,landa) alfalandaexp(-landax) ( (1+exp(-landa*x))^(-(alfa+1)) )

rlogis<-function(n,alfa,landa) { # tolid n ta az tozie logistic
u<-runif(n)
x<--(1/landa)*log(u^(-1/alfa)-1)
return(x) }

sxl<-function(x,l) sum(log(1+exp(-l*x)))

flanda<-function(x,l,a,b,alfa){
n<-length(x)
d1<-l^(n+a-1)
d2<- -l*(b+sum(x))-(alfa+1)sxl(x,l)
d3<-d1exp(d2)
return(d3) }
ralfa<-function(l,x,a,b){ # generate from conditional distribution alphas
n<-length(x)
d1<-n+a
d2<-b+sxl(x,l)
d3<-rgamma(1,d1,d2)
return(d3) }

#fab(2,1:6,4,.1)

mh<-function(nmh,landas,x,a,b,alfa){
set.seed<-seed
sample<-rep(0,nmh)
for(i in 1:nmh){
landa<-rnorm(1,landas,sigma2)
a.p1<-flanda(x,landa,a,b,alfa)
a.p2<-flanda(x,landas,a,b,alfa)
a.p3<-dnorm(landas,landa,sigma2)
a.p4<-dnorm(landa,landas,sigma2)
accept.prob<-(a.p1/a.p2)*(a.p3/a.p4)
ratio<-min(1,accept.prob)
u<-runif(1)
if(u<=ratio) sample[i]<-landa
else sample[i]<-landas
landas<-sample[i] }
return(mean(sample)) }

#observation<-rlogis(100,2,3)
#mh(10000,5,observation,2,3,alfa10)

GS<-function(N.GS,N.MH,landa1s,landa2s,x,y){
n<-length(x) ; m<-length(y)
alfa1<-alfa2<-landa1<-landa2<-rep(0,N.GS)
for(i in 1:N.GS){
alfa1[i]<-ralfa(landa1s,x,a10,b10)
alfa2[i]<-ralfa(landa2s,x,a20,b20)
landa1[i]<-mh(N.MH,landa1s,x,a30,b30,alfa1[i])
landa2[i]<-mh(N.MH,landa2s,x,a40,b40,alfa2[i])
landa1s<-landa1[i]
landa2s<-landa2[i] }
return(colMeans(cbind(alfa1,alfa2,landa1,landa2)[-(1:round(N.GS/4)),]))
}
n<-nsamplex
m<-nsampley

R12<-function(a1,a2){
G<-(2a1a2)/(a2*(a2+(2*a1)))
return(G) }

R23<-function(a1,a2){
G<-(6a1^2)/((a2+3a1)(a2+2a1))
return(G)}

R120<-R12(alfa10,alfa20)
R230<-R23(alfa10,alfa20)

N<-5000
R12ha<-rep(0,0)
bias12<-rep(0,0)
MSE12<-rep(0,0)
L12<-c(0,0)
CP12<-rep(0,0)
R23ha<-rep(0,0)
bias23<-rep(0,0)
MSE23<-rep(0,0)
L23<-c(0,0)
CP23<-rep(0,0)

for(j in 1:repeatn){
for(i in 1:N) {

observationx<-rlogis(nsamplex,alfa10,landa10)
observationy<-rlogis(nsampley,alfa20,landa20)
parameterestim<-GS(ngs,nmh0,landa1s,landa2s,observationx,observationy)

alfa1hat<-parameterestim[1]
alfa2hat<-parameterestim[2]
landa1hat<-parameterestim[3]
landa2hat<-parameterestim[4]

R12ha[i]<-R12(alfa1hat,alfa2hat)}

R12hat<-mean(R12ha)
bias12[j]<-R12hat-R120
MSE12[j]<-mean(R12ha-R120)^2
var12<-var(R12ha)
L12[j]<-21.96sqrt(var12)
CP12[j]<-sum((R120>=(R12hat-1.959sqrt(var12))) & (R120<=(R12hat+1.959sqrt(var12))))}

mean(bias12)
mean(MSE12)
mean(L12)
mean(CP12)


